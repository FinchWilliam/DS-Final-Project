{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import(\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertTokenizer, \n",
    "    BertForMaskedLM\n",
    "\n",
    ")\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from huggingface_hub import notebook_login\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"using device: {device}\")\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2908435\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/clean_food.pkl\")\n",
    "\n",
    "recipes = list(df['ingredients'].apply(lambda x: ', '.join(x)))\n",
    "dataset = Dataset.from_dict({'text': recipes})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # Or any other BERT variant\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df500d1453574fdbb6f91abe140bbc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2908435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, return_tensors = 'pt')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_dataset['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tokenized_dataset['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m token_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\datasets\\arrow_dataset.py:945\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[1;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[0;32m    944\u001b[0m mapping \u001b[38;5;241m=\u001b[39m arrow_typed_mapping\n\u001b[1;32m--> 945\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\datasets\\table.py:757\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03mConstruct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03m    `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pydict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\table.pxi:1968\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\table.pxi:6337\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\array.pxi:402\u001b[0m, in \u001b[0;36mpyarrow.lib.asarray\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\array.pxi:252\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\array.pxi:114\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\datasets\\arrow_writer.py:228\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    227\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\array.pxi:372\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\array.pxi:42\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\finch\\anaconda3\\envs\\test_env\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 17179869184 failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_dict({\"input_ids\": tokenized_dataset[\"input_ids\"],\n",
    "#                              \"attention_mask\": tokenized_dataset['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(examples):\n",
    "    input_ids = examples[\"input_ids\"]\n",
    "    batch_size, seq_length = input_ids.size()\n",
    "    labels = input_ids.clone()\n",
    "    # We sample a few tokens in each sequence for MLM training (15% by default)\n",
    "    probability_matrix = torch.full((batch_size, seq_length), 0.15)\n",
    "    special_tokens_mask = (input_ids == tokenizer.cls_token_id) | (input_ids == tokenizer.sep_token_id) | (input_ids == tokenizer.pad_token_id)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "    input_ids[masked_indices] = tokenizer.mask_token_id\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": examples[\"attention_mask\"], \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = 1000\n",
    "# masked_dataset = dataset.map(mask_tokens, batched=True)\n",
    "# masked_dataset.set_format(\"torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
